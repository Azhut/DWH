"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä Golden Snapshots –¥–ª—è —Ç–µ—Å—Ç–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞.
–ú–∏–Ω–∏–º–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: –ø—É—Ç–∏ –ø—Ä–∞–≤—è—Ç—Å—è –≤—Ä—É—á–Ω—É—é –≤ —ç—Ç–æ–º —Ñ–∞–π–ª–µ.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    1. –û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π TEST_FILE_PATH, FORM_ID –Ω–∏–∂–µ
    3. –ü—Ä–æ–≤–µ—Ä—å visual_report.xlsx –∏ .expected.json
"""
from pathlib import Path
from tests import FORM_ID
import asyncio
import json
from io import BytesIO
from typing import List, Dict, Any
from app.application.parsing.registry import get_parsing_strategy_registry
import pandas as pd
from fastapi import UploadFile
from app.application.upload.pipeline import build_default_pipeline, UploadPipelineContext
from app.core.dependencies import (
    get_file_service,
    get_form_service,
    get_sheet_service
)



# –ü—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤–æ–º—É —Ñ–∞–π–ª—É (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞)
TEST_FILE_PATH = Path("../../tests/fixtures/1fk/–ê–õ–ê–ü–ê–ï–í–°–ö 2020.xls")

# ID —Ñ–æ—Ä–º—ã
FORM_ID = FORM_ID

# –í–∏–∑—É–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç ‚Äî —Ä—è–¥–æ–º —Å–æ —Å–∫—Ä–∏–ø—Ç–æ–º
OUTPUT_EXCEL_PATH = Path(__file__).parent / "visual_report.xlsx"

# JSON-—Å–Ω–∞–ø—à–æ—Ç –¥–ª—è –∞–≤—Ç–æ—Ç–µ—Å—Ç–æ–≤ (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞)
OUTPUT_SNAPSHOT_PATH = Path("../../tests/fixtures/1fk_snapshots/–ê–õ–ê–ü–ê–ï–í–°–ö 2020.expected.json")

# –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏: –∫–∞–∫–∏–µ —è—á–µ–π–∫–∏ –ø—Ä–æ–≤–µ—Ä—è—Ç—å
# expected_value=None –æ–∑–Ω–∞—á–∞–µ—Ç "–ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–æ (–ª—é–±–æ–µ)"
CHECKPOINTS: List[Dict[str, Any]] = [
    {"row": "–í—Å–µ–≥–æ", "column": "–ì—Ä–∞—Ñ–∞ 3", "expected_value": None},
    {"row": "–ê–ª–∞–ø–∞–µ–≤—Å–∫", "column": "–ì—Ä–∞—Ñ–∞ 1", "expected_value": None},
]


async def run_pipeline_simulation(file_path: Path, form_id: str) -> UploadPipelineContext:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç —Ñ–∞–π–ª —á–µ—Ä–µ–∑ —Ä–µ–∞–ª—å–Ω—ã–π UploadPipeline.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞.
    """
    print(f"üöÄ –ó–∞–ø—É—Å–∫ pipeline –¥–ª—è —Ñ–∞–π–ª–∞: {file_path.name}")

    with open(file_path, "rb") as f:
        content = f.read()

    upload_file = UploadFile(
        filename=file_path.name,
        file=BytesIO(content),
        size=len(content),
    )

    form_service = get_form_service()
    form_info = await form_service.get_form_or_raise(form_id)

    file_service = get_file_service()
    # –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Å–Ω–∞–ø—à–æ—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º NoOp —Å–µ—Ä–≤–∏—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è,
    # —á—Ç–æ–±—ã –Ω–µ –≤–Ω–æ—Å–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ –ë–î –ø—Ä–∏ –ª–æ–∫–∞–ª—å–Ω–æ–º –∑–∞–ø—É—Å–∫–µ.
    class NoOpDataSaveService:
        async def process_and_save_all(self, file_model, flat_data=None):
            return None

        async def rollback(self, file_model, error: str):
            return None

        async def save_file(self, file_model):
            return None

    data_save_service = NoOpDataSaveService()
    sheet_service = get_sheet_service()

    get_parsing_strategy_registry(sheet_service=sheet_service)

    pipeline = build_default_pipeline(
        file_service=file_service,
        data_save_service=data_save_service,
    )

    ctx = UploadPipelineContext(
        file=upload_file,
        form_id=form_id,
        form_info=form_info,
    )

    try:
        await pipeline.run_for_file(ctx)
    except Exception as e:
        print(f"‚ö†Ô∏è Pipeline –∑–∞–≤–µ—Ä—à–∏–ª —Å –æ—à–∏–±–∫–æ–π (–≤–æ–∑–º–æ–∂–Ω–æ –æ–∂–∏–¥–∞–µ–º–æ–π): {e}")

    return ctx


def generate_visual_report(ctx: UploadPipelineContext, output_path: Path) -> None:
    """
    –°–æ–∑–¥–∞—ë—Ç Excel-—Ñ–∞–π–ª –¥–ª—è —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏.
    –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—á–∏—Ç–∞–µ—Ç—Å—è –∏–∑ ctx.flat_data (–∫–∞–∫ –≤ –ë–î).
    """
    print(f"üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞: {output_path}")
    output_path.parent.mkdir(parents=True, exist_ok=True)

    from scripts.golden_snapshot.table_builder import (
        build_multiindex_dataframe,
        build_summary_table,
        build_flat_data_preview,
        write_multiindex_excel,
    )

    # === –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –§–ê–ô–õ–£ ===
    flat_data = ctx.flat_data or []
    total_records = len(flat_data)

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –ª–∏—Å—Ç–∞–º (section)
    by_section: Dict[str, int] = {}
    for rec in flat_data:
        section = rec.section or "unknown"
        by_section[section] = by_section.get(section, 0) + 1

    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–Ω–∞—á–µ–Ω–∏—è–º
    numeric_values = [rec.value for rec in flat_data if isinstance(rec.value, (int, float))]
    numeric_sum = sum(numeric_values) if numeric_values else 0
    numeric_count = len(numeric_values)
    empty_count = sum(1 for rec in flat_data if rec.value is None or rec.value == "")

    with pd.ExcelWriter(output_path, engine="openpyxl") as writer:  # type: ignore
        # === –í–∫–ª–∞–¥–∫–∞ 0: –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –§–ê–ô–õ–£ ===
        overall_stats = {
            "–ú–µ—Ç—Ä–∏–∫–∞": [
                "–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π",
                "–ó–∞–ø–æ–ª–Ω–µ–Ω–æ",
                "–ü—É—Å—Ç—ã—Ö",
                "% –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç–∏",
                "–°—É–º–º–∞ —á–∏—Å–ª–æ–≤—ã—Ö",
                "–°—Ä–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ–≤–æ–µ",
                "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∏—Å—Ç–æ–≤",
            ],
            "–ó–Ω–∞—á–µ–Ω–∏–µ": [
                total_records,
                total_records - empty_count,
                empty_count,
                f"{((total_records - empty_count) / total_records * 100) if total_records > 0 else 0:.1f}%",
                round(numeric_sum, 2) if numeric_count > 0 else "N/A",
                round(numeric_sum / numeric_count, 2) if numeric_count > 0 else "N/A",
                len(by_section),
            ],
        }
        pd.DataFrame(overall_stats).to_excel(writer, sheet_name="–û–±—â–∞—è_—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", index=False)

        # === –í–∫–ª–∞–¥–∫–∞ 1: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ª–∏—Å—Ç–∞–º ===
        section_stats = [
            {"–õ–∏—Å—Ç": section, "–ó–∞–ø–∏—Å–µ–π": count}
            for section, count in sorted(by_section.items())
        ]
        pd.DataFrame(section_stats).to_excel(writer, sheet_name="–ü–æ_–ª–∏—Å—Ç–∞–º", index=False)

        # === –í–∫–ª–∞–¥–∫–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –ª–∏—Å—Ç—É ===
        if ctx.sheet_models:
            for sheet in ctx.sheet_models:
                sheet_name_clean = sheet.sheet_name[:30].replace(":", "_").replace("/", "_")

                sheet_data = {
                    "headers": sheet.headers if isinstance(sheet.headers, dict) else {
                        "horizontal": [], "vertical": [],
                    },
                    "data": sheet.data if isinstance(sheet.data, list) else [],
                }

                # –õ–∏—Å—Ç: –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞
                try:
                    pivot_df, n_levels = build_multiindex_dataframe(sheet_data, max_rows=100, max_cols=50)
                    temp_path = output_path.parent / f"_temp_{sheet_name_clean}.xlsx"
                    write_multiindex_excel(pivot_df, str(temp_path), sheet_name=f"{sheet_name_clean}_–¢–∞–±–ª–∏—Ü–∞")
                    temp_df = pd.read_excel(temp_path, sheet_name=0)
                    temp_df.to_excel(writer, sheet_name=f"{sheet_name_clean}_–¢–∞–±–ª–∏—Ü–∞", index=False)
                    temp_path.unlink()
                    print(
                        f"  ‚úÖ {sheet_name_clean}_–¢–∞–±–ª–∏—Ü–∞: {pivot_df.shape[0]} √ó {pivot_df.shape[1]} ({n_levels} —É—Ä–æ–≤–Ω–µ–π)")
                except Exception as e:
                    print(f"  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É –¥–ª—è {sheet_name_clean}: {e}")

                # –õ–∏—Å—Ç: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                try:
                    summary_df = build_summary_table(sheet_data)
                    summary_df.to_excel(writer, sheet_name=f"{sheet_name_clean}_–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", index=False)
                    print(f"  ‚úÖ {sheet_name_clean}_–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ")
                except Exception as e:
                    print(f"  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è {sheet_name_clean}: {e}")

                # –õ–∏—Å—Ç: –ü–ª–æ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
                try:
                    flat_df = build_flat_data_preview(sheet_data, max_rows=200)
                    if not flat_df.empty:
                        flat_df.to_excel(writer, sheet_name=f"{sheet_name_clean}_–î–∞–Ω–Ω—ã–µ", index=False)
                        print(f"  ‚úÖ {sheet_name_clean}_–î–∞–Ω–Ω—ã–µ: {len(flat_df)} –∑–∞–ø–∏—Å–µ–π")
                    else:
                        print(f"  ‚ö†Ô∏è {sheet_name_clean}_–î–∞–Ω–Ω—ã–µ: –ø—É—Å—Ç–æ–π –ª–∏—Å—Ç")
                except Exception as e:
                    print(f"  ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è {sheet_name_clean}: {e}")

    print(f"‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")
    print(f"üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π (flat_data): {total_records}")


def generate_snapshot(
        ctx: UploadPipelineContext,
        output_path: Path,
        checkpoints: List[Dict[str, Any]],
) -> None:
    """
    –°–æ–∑–¥–∞—ë—Ç JSON-—Å–Ω–∞–ø—à–æ—Ç –¥–ª—è –∞–≤—Ç–æ—Ç–µ—Å—Ç–æ–≤.
    –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å—á–∏—Ç–∞–µ—Ç—Å—è –∏–∑ ctx.flat_data (–∫–∞–∫ –≤ –ë–î).
    Checkpoints —Ö—Ä–∞–Ω—è—Ç —Ç–æ–ª—å–∫–æ –æ–∂–∏–¥–∞–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–±–µ–∑ actual/match).
    """
    print(f"üì∏ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è JSON-—Å–Ω–∞–ø—à–æ—Ç–∞: {output_path}")
    output_path.parent.mkdir(parents=True, exist_ok=True)

    from datetime import datetime

    # === –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–ó flat_data (–∫–∞–∫ –≤ –ë–î) ===
    flat_data = ctx.flat_data or []
    total_records = len(flat_data)

    # –ò–∑–≤–ª–µ–∫–∞–µ–º year –∏ reporter –∏–∑ –ø–µ—Ä–≤–æ–π –∑–∞–ø–∏—Å–∏
    year = flat_data[0].year if flat_data and flat_data[0].year else None
    reporter = flat_data[0].reporter if flat_data and flat_data[0].reporter else None

    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –ª–∏—Å—Ç–∞–º (section)
    by_section: Dict[str, int] = {}
    for rec in flat_data:
        section = rec.section or "unknown"
        by_section[section] = by_section.get(section, 0) + 1

    snapshot = {
        "meta": {
            "generated_at": datetime.now().isoformat(),
            "file_name": ctx.file.filename,
            "form_id": ctx.form_id,
            "description": "Golden Master. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–∑ ctx.flat_data (–∫–∞–∫ –≤ –ë–î).",
        },
        "stats": {
            "year": year,
            "reporter": reporter,
            "total_flat_records": total_records,
            "total_sheets": len(by_section),
            "by_section": by_section,
        },
        "sheets": [],
        "checkpoints": [],
    }

    # === –î–µ—Ç–∞–ª–∏ –ø–æ –ª–∏—Å—Ç–∞–º (–∏–∑ sheet_models –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã) ===
    if ctx.sheet_models:
        for sheet in ctx.sheet_models:
            sheet_info = {
                "name": sheet.sheet_name,
                "flat_records_count": by_section.get(sheet.sheet_name, 0),  # –ò–∑ flat_data
                "headers_count": {
                    "horizontal": len(sheet.headers.get("horizontal", [])),
                    "vertical": len(sheet.headers.get("vertical", [])),
                },
                # –û—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏, –Ω–æ –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤ —Ç–µ—Å—Ç–µ
                "first_horizontal": (
                    sheet.headers.get("horizontal", [""])[0]
                    if sheet.headers.get("horizontal") else ""
                ),
                "first_vertical": (
                    sheet.headers.get("vertical", [""])[0]
                    if sheet.headers.get("vertical") else ""
                ),
            }
            snapshot["sheets"].append(sheet_info)

            # === –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ ===
            # –•—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ –æ–∂–∏–¥–∞–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–∫–∞–∫ –≤ –ë–î)
            for cp in checkpoints:
                found = None
                for rec in flat_data:
                    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ section (sheet_name), row, column
                    if (rec.section == cp.get("sheet") and
                            rec.row == cp["row"] and
                            rec.column == cp["column"]):
                        found = rec.value
                        break

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ, –±–ª–∏–∑–∫–æ–º –∫ FlatDataRecord (–±–µ–∑ file_id –∏ form)
                snapshot["checkpoints"].append({
                    "year": year,
                    "reporter": reporter,
                    "section": cp.get("sheet"),
                    "row": cp["row"],
                    "column": cp["column"],
                    "value": found,  # –¢–æ–ª—å–∫–æ –æ–∂–∏–¥–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                })

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(snapshot, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –°–Ω–∞–ø—à–æ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")


async def main():
    if not TEST_FILE_PATH.exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {TEST_FILE_PATH.absolute()}")
        print("üí° –ü—Ä–æ–≤–µ—Ä—å –ø—É—Ç—å –∏ –∑–∞–ø—É—Å—Ç–∏ —Å–∫—Ä–∏–ø—Ç –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞")
        return

    # 1. –ó–∞–ø—É—Å–∫ pipeline
    ctx = await run_pipeline_simulation(TEST_FILE_PATH, FORM_ID)

    if ctx.failed:
        print(f"‚ùå Pipeline —É–ø–∞–ª: {ctx.error}")
        print("–ò—Å–ø—Ä–∞–≤—å –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Å–Ω–∞–ø—à–æ—Ç–∞!")
        return

    # 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Excel –¥–ª—è —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
    generate_visual_report(ctx, OUTPUT_EXCEL_PATH)

    # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–Ω–∞–ø—à–æ—Ç–∞ (–±–µ–∑ –ø–∞—É–∑—ã)
    generate_snapshot(ctx, OUTPUT_SNAPSHOT_PATH, CHECKPOINTS)

    print("\n" + "=" * 70)
    print("‚úÖ –ì–û–¢–û–í–û!")
    print("=" * 70)
    print(f"1. –û—Ç–∫—Ä–æ–π: {OUTPUT_EXCEL_PATH.absolute()}")
    print(f"2. –ü—Ä–æ–≤–µ—Ä—å –≤–∫–ª–∞–¥–∫—É '–û–±—â–∞—è_—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞' –∏ '–ü–æ_–ª–∏—Å—Ç–∞–º'")
    print(f"3. –ï—Å–ª–∏ –≤—Å—ë –æ–∫ ‚Äî –∑–∞–∫–æ–º–º–∏—Ç—å: {OUTPUT_SNAPSHOT_PATH}")


if __name__ == "__main__":
    asyncio.run(main())